---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a second-year PhD student (2023-) in the Department of Computer Science at the University of Illinois Chicago (UIC), where I am fortunate to be supervised by [Prof. Yan Yan](https://tomyan555.github.io/). Prior to joining UIC, I spent a year at Illinois Institute of Technology (Illinois Tech). I hold both a bachelor's and a master's degree from Shanghai Jiao Tong University, where I was advised by [Prof. Junchi Yan](https://thinklab.sjtu.edu.cn/). 

My research interests include Machine Learning Efficiency, Imbalanced Learning, and 3D vision. Most of the publications can be accessed [here](https://scholar.google.com/citations?user=vRXYQvYAAAAJ&hl=en).


# üî• News
- *2024.11*: &nbsp; Serving as a web co-chair for ICMR 2025
- *2024.06*: &nbsp; 1 paper accepted to NeurIPS 2024

# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/ptq4dit.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[PTQ4DiT: Post-training Quantization for Diffusion Transformers](https://arxiv.org/abs/2405.16005)

Junyi Wu\*, **Haoxuan Wang\***, Yuzhang Shang, Mubarak Shah, Yan Yan

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=vRXYQvYAAAAJ&citation_for_view=vRXYQvYAAAAJ:d1gkVwhDpl0C) <strong><span class='show_paper_citations' data='vRXYQvYAAAAJ:d1gkVwhDpl0C'></span></strong>
- Pioneering work for DiT quantization. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><img src='images/quest.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[QuEST: Low-bit Diffusion Model Quantization via Efficient Selective Finetuning](https://arxiv.org/abs/2402.03666)

**Haoxuan Wang**, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Junchi Yan, Yan Yan

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=vRXYQvYAAAAJ&citation_for_view=vRXYQvYAAAAJ:2osOgNQ5qMEC) <strong><span class='show_paper_citations' data='vRXYQvYAAAAJ:2osOgNQ5qMEC'></span></strong> | ![GitHub Repo stars](https://img.shields.io/github/stars/hatchetProject/QuEST)

- Parameter efficient finetuning method for diffusion model quantization. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCAI 2023</div><img src='images/drst.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Learning Calibrated Uncertainties for Domain Shift: A Distributionally Robust Learning Approach](https://arxiv.org/pdf/2010.05784)

**Haoxuan Wang**, Zhiding Yu, Yisong Yue, Animashree Anandkumar, Anqi Liu and Junchi Yan

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=vRXYQvYAAAAJ&citation_for_view=vRXYQvYAAAAJ:u-x6o8ySG0sC) <strong><span class='show_paper_citations' data='vRXYQvYAAAAJ:u-x6o8ySG0sC'></span></strong>
- A novel framework for learning calibrated uncertainties under domain shifts.
</div>
</div>


# üéñ Honors and Awards
- *2021-2022* First Award of SJTU scholarship
- *2020.06* Graduation with honor, University Graduate Excellence Award of SJTU
- *2019.11* First Award of Zhiyuan Research Program

<!--# üìñ Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. --!>

<!--# üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) --!>

<!--# üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. --!>
